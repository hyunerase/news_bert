# Naver News Full-Text Collector

## 📖 개요

이 프로젝트는 네이버 검색 API를 활용하여 특정 키워드에 대한 뉴스 기사 메타데이터를 수집하고, 각 기사의 원문 링크에 접속하여 전체 본문을 스크래핑하는 파이썬 기반의 데이터 수집기입니다.


## ✨ 주요 특징

- **네이버 뉴스 API 연동**: 검색어를 기반으로 뉴스 기사의 제목, 요약, 원문 링크 등의 메타데이터를 안정적으로 수집합니다.
- **다중 레이어 스크래핑**: 기사 본문 추출 성공률을 높이기 위해 아래와 같은 순차적 전략을 사용합니다.
  1. **`trafilatura`**: 가장 먼저 시도되는 고성능 웹 콘텐츠 추출 라이브러리.
  2. **`readability-lxml`**: 1차 시도 실패 시 사용되는 대체 라이브러리.
  3. **Fallback**: 모든 지능형 추출 실패 시, `<body>` 태그의 전체 텍스트를 추출하는 최후의 수단.
- **중단 처리**: 데이터 수집 중 `Ctrl+C`를 누르면, 프로세스가 즉시 종료되지 않고 그때까지 수집된 데이터를 안전하게 파일로 저장합니다.
- **동적 파일명 생성**: 실행 시점의 타임스탬프와 검색어를 조합하여 고유한 파일명을 생성하므로, 기존 데이터를 덮어쓸 염려가 없습니다.
- **다양한 출력 포맷**: 수집된 데이터는 분석에 용이한 `CSV`와 `Parquet` 두 가지 형식으로 동시에 저장됩니다.

## 📂 프로젝트 구조

```
naver_api/
├── src/
│   ├── __init__.py         # src 폴더를 패키지로 인식
│   ├── collector.py      # API 호출 및 스크래핑 조율
│   ├── config.py         # 고정 설정값 관리
│   ├── scraper.py        # 실제 본문을 스크래핑하는 핵심 로직
│   └── utils.py          # API 키 로드, 날짜 변환 등 헬퍼 함수
├── main.py               # 프로그램의 메인 실행 파일
├── .env                  # API 키를 저장하는 파일 (사용자가 생성)
├── .gitignore            # Git 추적 제외 목록
└── requirements.txt      # 프로젝트 의존성 라이브러리 목록
```

## 🚀 설치 및 설정 방법

1.  **의존성 라이브러리 설치**
    터미널에서 아래 명령어를 실행하여 필요한 라이브러리를 모두 설치합니다.
    ```sh
    pip install -r requirements.txt
    ```

2.  **API 키 설정**
    프로젝트 루트 폴더에 `.env` 파일을 생성하고, 아래와 같이 네이버 검색 API에서 발급받은 Client ID와 Client Secret을 입력합니다.
    ```
    # .env 파일 내용
    NAVER_CLIENT_ID=여기에_클라이언트_ID를_입력하세요
    NAVER_CLIENT_SECRET=여기에_클라이언트_시크릿을_입력하세요
    ```

## 💻 사용법

1.  **수집 설정**
    `main.py` 파일 상단의 `main()` 함수 내에서 수집하려는 `QUERY`(검색어), `MAX_ITEMS`(최대 기사 수) 등을 필요에 맞게 수정합니다.

2.  **프로그램 실행**
    터미널에서 아래 명령어를 입력하여 데이터 수집을 시작합니다.
    ```sh
    python main.py
    ```

3.  **결과 확인**
    수집이 완료되면 `out/` 폴더에 `[타임스탬프]_[검색어].csv`와 `[타임스탬프]_[검색어].parquet` 파일이 생성됩니다.

## 📝 의존성

- `requests`
- `beautifulsoup4`
- `lxml`
- `trafilatura`
- `readability-lxml`
- `pandas`
- `pyarrow` (Parquet 저장을 위해)